# ruff: noqa: E731
# AUTOGENERATED KERNEL (LLM)
# Source: LLM-generated candidate implementation
# Status: Experimental / uncurated
# Expectation: Correctness-first, performance not representative
import math

import torch
import torch.nn as nn
import triton
import triton.language as tl


# -------------------------------------------------------------------
# Triton kernels with autotune for Intel Arc B580
# -------------------------------------------------------------------
@triton.autotune(
    configs=[
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=4, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=4, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 64}, num_warps=8, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 64}, num_warps=4, num_stages=3
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 64}, num_warps=8, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 64}, num_warps=8, num_stages=2
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def _linear_add_kernel(
    x_ptr,
    w_ptr,
    bias_ptr,
    add_ptr,
    out_ptr,
    M,
    N,
    K,
    stride_xm,
    stride_xk,
    stride_w0,
    stride_w1,
    stride_o0,
    stride_o1,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Fused linear (GEMM) + bias + add_value in FP32.
    x: [M, K], w: [N, K], bias: [N], add_value: [N], out: [M, N]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    row_off = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    col_off = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    row_mask = row_off < M
    col_mask = col_off < N

    # accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # loop over K
    for k0 in range(0, K, BLOCK_K):
        k_off = k0 + tl.arange(0, BLOCK_K)
        k_mask = k_off < K

        # load x [BLOCK_M, BLOCK_K]
        x_ptrs = x_ptr + row_off[:, None] * stride_xm + k_off[None, :] * stride_xk
        x_block = tl.load(
            x_ptrs, mask=(row_mask[:, None] & k_mask[None, :]), other=0.0
        ).to(tl.float32)

        # load w [BLOCK_N, BLOCK_K]
        w_ptrs = w_ptr + col_off[:, None] * stride_w0 + k_off[None, :] * stride_w1
        w_block = tl.load(
            w_ptrs, mask=(col_mask[:, None] & k_mask[None, :]), other=0.0
        ).to(tl.float32)
        w_block = w_block.T  # [BLOCK_K, BLOCK_N]

        acc = tl.dot(x_block, w_block, acc)

    # load bias & add_value, broadcast, accumulate
    b = tl.load(bias_ptr + col_off, mask=col_mask, other=0.0).to(tl.float32)
    a = tl.load(add_ptr + col_off, mask=col_mask, other=0.0).to(tl.float32)
    acc = acc + b[None, :] + a[None, :]

    # store result
    out_ptrs = out_ptr + row_off[:, None] * stride_o0 + col_off[None, :] * stride_o1
    write_mask = row_mask[:, None] & col_mask[None, :]
    tl.store(out_ptrs, acc, mask=write_mask)


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_SIZE": 256}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 512}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 1024}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 1024}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_SIZE": 2048}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_SIZE": 4096}, num_warps=8, num_stages=2),
    ],
    key=["numel"],
)
@triton.jit
def _activation_chain_kernel(inp_ptr, out_ptr, numel, BLOCK_SIZE: tl.constexpr):
    """
    Fused Swish -> Tanh -> GELU (exact) -> HardTanh on FP32 data.
    """
    pid = tl.program_id(0)
    start = pid * BLOCK_SIZE
    offs = start + tl.arange(0, BLOCK_SIZE)
    mask = offs < numel

    x = tl.load(inp_ptr + offs, mask=mask, other=0.0)

    # 1) Swish
    sig = 1.0 / (1.0 + tl.exp(-x))
    s_swish = x * sig

    # 2) Tanh
    s_tanh = 2.0 / (1.0 + tl.exp(-2.0 * s_swish)) - 1.0

    # 3) Exact GELU
    inv_sqrt2 = 0.70710678118654752440
    y_gelu = 0.5 * s_tanh * (1.0 + tl.math.erf(s_tanh * inv_sqrt2))

    # 4) HardTanh clamp [-1, 1]
    y = tl.maximum(y_gelu, -1.0)
    y = tl.minimum(y, 1.0)

    tl.store(out_ptr + offs, y, mask=mask)


# -------------------------------------------------------------------
# Model class for KernelBench harness
# -------------------------------------------------------------------
class Model(nn.Module):
    """
    Model that performs a matrix multiplication, adds a value,
    applies Swish, Tanh, GELU, and Hardtanh activation functions.

    Uses native Triton kernels for all operations.
    """

    def __init__(self, in_features, out_features, add_value_shape):
        super(Model, self).__init__()

        self.in_features = in_features
        self.out_features = out_features

        # Handle add_value_shape as list or tuple
        if isinstance(add_value_shape, list):
            add_value_shape = tuple(add_value_shape)
        self.add_value_shape = add_value_shape

        # Linear layer weights: weight [out_features, in_features], bias [out_features]
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.empty(out_features))

        # Add value parameter
        self.add_value = nn.Parameter(torch.randn(add_value_shape))

        # Initialize weights
        self._reset_parameters()

    def _reset_parameters(self):
        # Kaiming uniform initialization (same as nn.Linear)
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        if fan_in != 0:
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x):
        """
        Forward pass using Triton kernels.
        Input x: [batch, in_features] - can be float16 or float32
        Output: [batch, out_features] - same dtype as input
        """
        # Get input properties
        input_dtype = x.dtype
        device = x.device

        # Ensure contiguous
        if not x.is_contiguous():
            x = x.contiguous()

        # Get dimensions
        M, K = x.shape
        N = self.out_features

        # Ensure weights are contiguous
        weight = self.weight
        bias = self.bias
        add_value = self.add_value.view(-1)  # Flatten to [N]

        if not weight.is_contiguous():
            weight = weight.contiguous()
        if not bias.is_contiguous():
            bias = bias.contiguous()
        if not add_value.is_contiguous():
            add_value = add_value.contiguous()

        # Intermediate buffer in float32 for numerical stability
        intermediate = torch.empty((M, N), device=device, dtype=torch.float32)

        # Get strides
        sxm, sxk = x.stride(0), x.stride(1)
        sw0, sw1 = weight.stride(0), weight.stride(1)
        so0, so1 = intermediate.stride(0), intermediate.stride(1)

        # Launch linear+add kernel with autotune grid
        def grid_linear(meta):
            return (triton.cdiv(M, meta["BLOCK_M"]), triton.cdiv(N, meta["BLOCK_N"]))

        _linear_add_kernel[grid_linear](
            x,
            weight,
            bias,
            add_value,
            intermediate,
            M,
            N,
            K,
            sxm,
            sxk,
            sw0,
            sw1,
            so0,
            so1,
        )

        # Output buffer in float32
        out = torch.empty_like(intermediate)
        numel = M * N

        # Launch activation chain kernel with autotune grid
        def grid_activation(meta):
            return (triton.cdiv(numel, meta["BLOCK_SIZE"]),)

        _activation_chain_kernel[grid_activation](
            intermediate,
            out,
            numel,
        )

        # Convert back to input dtype if needed
        if input_dtype != torch.float32:
            out = out.to(input_dtype)

        return out


# -------------------------------------------------------------------
# KernelBench harness functions
# -------------------------------------------------------------------
batch_size = 1024
in_features = 4096
out_features = 4096
add_value_shape = (out_features,)


def get_inputs():
    return [torch.randn(batch_size, in_features, dtype=torch.float16)]


def get_init_inputs():
    return [in_features, out_features, add_value_shape]
